{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0844ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done.\n",
      "Train size: 900 | Test size: 100\n",
      "Train A(GT): saved=900, warn=0, miss=0\n",
      "Train B(LR): saved=900, warn=0, miss=0\n",
      "Test  A(GT): saved=100,  warn=0,  miss=0\n",
      "Test  B(LR): saved=100,  warn=0,  miss=0\n",
      "\n",
      "Train dir: /home/data1/musong/workspace/python/cyclegan/datasets/IXI/train\n",
      "Test  dir: /home/data1/musong/workspace/python/cyclegan/datasets/IXI/test\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# --------- Config ---------\n",
    "GT_DIR = \"/home/data1/musong/workspace/2025/8/08-20/tr/data/IXI_sim/data\"\n",
    "LR_DIR = \"/home/data1/musong/workspace/2025/8/08-20/tr/data/IXI_sim/final_rxyacq_ROFFT\"\n",
    "\n",
    "OUT_ROOT = \"/home/data1/musong/workspace/python/cyclegan/datasets/IXI\"\n",
    "TRAIN_DIR = os.path.join(OUT_ROOT, \"train\")\n",
    "TEST_DIR  = os.path.join(OUT_ROOT, \"test\")\n",
    "\n",
    "SPLIT_TRAIN = 0.9     # 9:1 split\n",
    "SEED = 20250823       # deterministic split\n",
    "# --------------------------\n",
    "\n",
    "def ensure_dirs():\n",
    "    for base in (TRAIN_DIR, TEST_DIR):\n",
    "        os.makedirs(os.path.join(base, \"A\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(base, \"B\"), exist_ok=True)\n",
    "\n",
    "def normalize_to_uint8(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Min-max normalize array to [0,255] uint8 safely.\"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    vmin = float(arr.min())\n",
    "    vmax = float(arr.max())\n",
    "    if vmax > vmin:\n",
    "        arr = (arr - vmin) / (vmax - vmin)\n",
    "    else:\n",
    "        arr = np.zeros_like(arr, dtype=np.float32)\n",
    "    return (arr * 255.0).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "def to_rgb(arr_2d: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Replicate a single-channel (H,W) image into (H,W,3).\"\"\"\n",
    "    arr_u8 = normalize_to_uint8(arr_2d)\n",
    "    return np.stack([arr_u8, arr_u8, arr_u8], axis=-1)\n",
    "\n",
    "def last_data_key(d: dict) -> str:\n",
    "    \"\"\"Return the last non-metadata key from a loadmat() dict.\"\"\"\n",
    "    keys = [k for k in d.keys() if not (isinstance(k, str) and k.startswith(\"__\"))]\n",
    "    if not keys:\n",
    "        return list(d.keys())[-1]\n",
    "    return keys[-1]\n",
    "\n",
    "def load_mat_last_key(path: str) -> np.ndarray:\n",
    "    \"\"\"Load a .mat and return ndarray stored under the last non-metadata key.\"\"\"\n",
    "    d = sio.loadmat(path)\n",
    "    key = last_data_key(d)\n",
    "    data = d[key]\n",
    "    if not isinstance(data, np.ndarray):\n",
    "        raise ValueError(f\"Data at last key is not an ndarray in {path}\")\n",
    "    return data\n",
    "\n",
    "def to_2d(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Squeeze singleton dims; ensure result is 2D.\n",
    "    Accepts (W,H), (1,W,H), or (W,H,1). Raises if not 2D after squeeze.\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    arr = np.squeeze(arr)\n",
    "    if arr.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D after squeeze, got shape {arr.shape}\")\n",
    "    return arr\n",
    "\n",
    "def save_png(img_rgb: np.ndarray, out_path: str):\n",
    "    Image.fromarray(img_rgb).save(out_path)\n",
    "\n",
    "def split_stems(stems):\n",
    "    \"\"\"Deterministic 9:1 split with a fixed seed.\"\"\"\n",
    "    stems = list(stems)\n",
    "    random.Random(SEED).shuffle(stems)\n",
    "    n_train = int(len(stems) * SPLIT_TRAIN)\n",
    "    return stems[:n_train], stems[n_train:]\n",
    "\n",
    "def process_one(stem: str, dst_base: str,\n",
    "                gt_counts: dict, lr_counts: dict):\n",
    "    gt_path = os.path.join(GT_DIR, stem + \".mat\")\n",
    "    lr_path = os.path.join(LR_DIR, stem + \".mat\")\n",
    "\n",
    "    # --- GT -> A ---\n",
    "    if os.path.exists(gt_path):\n",
    "        try:\n",
    "            gt_raw = load_mat_last_key(gt_path)\n",
    "            gt_2d  = to_2d(gt_raw)\n",
    "            gt_rgb = to_rgb(gt_2d)\n",
    "            save_png(gt_rgb, os.path.join(dst_base, \"A\", stem + \".png\"))\n",
    "            gt_counts[\"saved\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[GT  WARN] {stem}: {e}\")\n",
    "            gt_counts[\"warn\"] += 1\n",
    "    else:\n",
    "        # Not fatal; dataset A/B can be unpaired\n",
    "        gt_counts[\"miss\"] += 1\n",
    "\n",
    "    # --- LR -> B (magnitude) ---\n",
    "    if os.path.exists(lr_path):\n",
    "        try:\n",
    "            lr_raw = load_mat_last_key(lr_path)\n",
    "            lr_2d  = to_2d(lr_raw)\n",
    "            lr_mag = np.abs(lr_2d)\n",
    "            lr_rgb = to_rgb(lr_mag)\n",
    "            save_png(lr_rgb, os.path.join(dst_base, \"B\", stem + \".png\"))\n",
    "            lr_counts[\"saved\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[LR  WARN] {stem}: {e}\")\n",
    "            lr_counts[\"warn\"] += 1\n",
    "    else:\n",
    "        lr_counts[\"miss\"] += 1\n",
    "\n",
    "def main():\n",
    "    ensure_dirs()\n",
    "\n",
    "    gt_files = sorted([f for f in os.listdir(GT_DIR) if f.endswith(\".mat\")])\n",
    "    lr_files = sorted([f for f in os.listdir(LR_DIR) if f.endswith(\".mat\")])\n",
    "\n",
    "    gt_stems = {Path(f).stem for f in gt_files}\n",
    "    lr_stems = {Path(f).stem for f in lr_files}\n",
    "    all_stems = sorted(gt_stems | lr_stems)\n",
    "\n",
    "    train_stems, test_stems = split_stems(all_stems)\n",
    "\n",
    "    counts = {\n",
    "        \"train_gt\": {\"saved\": 0, \"warn\": 0, \"miss\": 0},\n",
    "        \"train_lr\": {\"saved\": 0, \"warn\": 0, \"miss\": 0},\n",
    "        \"test_gt\":  {\"saved\": 0, \"warn\": 0, \"miss\": 0},\n",
    "        \"test_lr\":  {\"saved\": 0, \"warn\": 0, \"miss\": 0},\n",
    "    }\n",
    "\n",
    "    # Process train\n",
    "    for stem in train_stems:\n",
    "        process_one(stem, TRAIN_DIR, counts[\"train_gt\"], counts[\"train_lr\"])\n",
    "\n",
    "    # Process test\n",
    "    for stem in test_stems:\n",
    "        process_one(stem, TEST_DIR, counts[\"test_gt\"], counts[\"test_lr\"])\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n✅ Done.\")\n",
    "    print(f\"Train size: {len(train_stems)} | Test size: {len(test_stems)}\")\n",
    "    print(f\"Train A(GT): saved={counts['train_gt']['saved']}, warn={counts['train_gt']['warn']}, miss={counts['train_gt']['miss']}\")\n",
    "    print(f\"Train B(LR): saved={counts['train_lr']['saved']}, warn={counts['train_lr']['warn']}, miss={counts['train_lr']['miss']}\")\n",
    "    print(f\"Test  A(GT): saved={counts['test_gt']['saved']},  warn={counts['test_gt']['warn']},  miss={counts['test_gt']['miss']}\")\n",
    "    print(f\"Test  B(LR): saved={counts['test_lr']['saved']},  warn={counts['test_lr']['warn']},  miss={counts['test_lr']['miss']}\")\n",
    "    print(f\"\\nTrain dir: {TRAIN_DIR}\")\n",
    "    print(f\"Test  dir: {TEST_DIR}\")\n",
    "\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
